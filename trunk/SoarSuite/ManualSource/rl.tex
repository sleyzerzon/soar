\chapter{Reinforcement Learning}
\label{RL}
\index{reinforcement learning}
\index{preference!numeric-indifferent}
\index{rl}

The reinforcement learning (RL) mechanism in Soar tunes the values of numeric indifferent preferences for each operator to represent how beneficial the operator is to execute.
More precisely, the numeric indifferent preference values are adjusted to approximate the total discounted future reward the agent should expect from executing an operator, given a fixed policy of future decisions.

The main components of Soar's RL mechanism are (\ref{RL-rules}) representation of operator selection knowledge; (\ref{RL-reward}) reward representation; and (\ref{RL-algo}) the update algorithm. 
The detailed behavior of the RL mechanism is determined by numerous parameters that can be controlled and configured via the \soarb{rl} command. Please refer to the documentation for that command in section \ref{rl} on page \pageref{rl}.

We assume that the reader is familiar with basic reinforcement learning concepts and notation. If not, we recommend first reading \emph{Reinforcement Learning: An Introduction} (1998) by Richard S. Sutton and Andrew G. Barto.

\section{RL Rules}
\label{RL-rules}

The RL mechanism changes agent behavior by updating numeric indifferent preference values asserted by specially formulated productions called \emph{RL rules}.
RL rules are identified by syntax.
A production is a RL rule if and only if its left hand side tests for a proposed operator, its right hand side asserts a single numeric indifferent preference, and it is not a template rule (see \ref{RL-templates}).
These constraints ease the technical requirements of identifying/updating RL rules and makes it easy for the agent programmer to add/maintain RL capabilities within an agent.

The following is an RL rule:

\begin{verbatim}
sp {example*rl*rule
   (state <s> ^name task-name
	          ^operator <o> +)
   (<o> ^name move
	    ^direction left)
-->
   (<s> ^operator <o> = 1.5)
}
\end{verbatim}

Note that the LHS of the rule can test for anything as long as it contains a test for a proposed operator.
The RHS is constrained to exactly one action: asserting a numeric indifferent preference for the proposed operator.

The following are not RL rules:

\begin{verbatim}
sp {multiple*preferences
   (state <s> ^operator <o> +)
-->
   (<s> ^operator <o> = 5, >)
}
\end{verbatim}  \vspace{12pt}

\begin{verbatim}
sp {variable*binding
    (state <s> ^operator <o> +
               ^value <v>)
-->
    (<s> ^operator <o> = <v>)
}
\end{verbatim}  \vspace{12pt}

The first rule proposes multiple preferences for the proposed operator and thus does not comply with the rule format.
The second rule does not comply because it does not provide a \emph{constant} for the numeric indifferent preference value.

\section{Reward Representation}
\label{RL-reward}

Operator selection knowledge for an operator at a particular state is affected only by reward at that state.

RL updates are driven by reward signals.
In Soar, these reward signals are fed to the RL mechanism through a working memory link called the \soarb{reward-link}.
Each state in Soar's goal stack is automatically populated with a \soarb{reward-link} structure upon creation.
Soar will check this structure for a numeric reward signal for the last operator executed in the associated state at the beginning of every decision phase.
Reward is also counted when the agent is halted or a substate is retracted.
% What happens when an agent with multiple states is halted? Do the rewards in the substates get counted?

In order to be recognized, the reward signal must follow this pattern:

\begin{verbatim}
(<r1> ^reward <r2>)
(<r2> ^value [val])
\end{verbatim}

where \verb=<r1>= is the \soarb{reward-link} identifier, \verb=<r2>= is some intermediate identifier, and \verb=[val]= is any constant numeric value.
Any structure that does not match this pattern are ignored.
If there are multiple matching WMEs, their values are summed into a single reward signal.

As an example, consider the following state:

\begin{verbatim}
(S1 ^reward-link R1)
  (R1 ^reward R2)
    (R2 ^value 1.0)
    (R2 ^source environment)
  (R1 ^reward R3)
    (R3 ^value -0.2)
    (R3 ^source intrinsic)
\end{verbatim}  

In this state, there are two reward signals with values 1.0 and -0.2.
They will be summed together for a total reward of 0.8 and this will be the value given to the RL update algorithm.
The \verb=(R2 ^source environment)= and \verb=(R3 ^source intrinsic)= WMEs are not counted as rewards or special in any way, but were added by the agent to keep track of where the rewards came from.

Note that the \soarb{reward-link} is not part of the \soarb{io} structure and is not modified directly by the environment.
Reward information from the environment should be copied, via rules, from the \soarb{input-link} to the \soarb{reward-link}.
Also note that when counting rewards, Soar simply scans the \soarb{reward-link} and sums the values of all valid reward WMEs.
The WMEs are not modified and no bookkeeping is done to keep track of previously seen WMEs.
This means that reward WMEs that exist for multiple decision cycles such as o-supported WMEs will be counted multiple times.

\section{Updating RL Rule Values}
\label{RL-algo}

The RL mechanism implements Temporal Difference (TD) learning: the estimated value of selecting an operator in the present state is \emph{updated} in the direction of a later estimate of this quantity.
This update is computed according to the following equation:

\begin{center}
Current Estimate += $\alpha$(Target Estimate - Current Estimate)
\end{center}

where $\alpha$ is the \emph{learning rate}.

The update is computed in decision phase \soarb{n+1} and then is apportioned to the RL rules that fired for the operator selected in decision phase \soarb{n} in a least mean squares (LMS), gradient-descent fashion.

Updating an RL rule involves changing the value of its numeric indifference preference.
For example:

\begin{enumerate}

\item In decision phase \soarb{n}, RL rules \soarb{rl-1} and \soarb{rl-2} fire for operator \soarb{O2}.
	They have the following numeric indifferent preferences:
\begin{verbatim}
   rl-1: (<s> ^operator <o> = 2.3)
   rl-2: (<s> ^operator <o> = -1)
\end{verbatim}  

\item \soarb{O2} is selected.

\item In decision phase \soarb{n+1}, update \soarb{0.2} is computed.

\item \soarb{rl-1} and \soarb{rl-2} are updated with the following numeric indifferent preferences:
\begin{verbatim}
   rl-1: (<s> ^operator <o> = 2.4)
   rl-2: (<s> ^operator <o> = -0.9)
\end{verbatim}

Note that the update value is divided amongst and applied equally to all contributing numeric indifference preferences originating from RL rules.

\end{enumerate}

\subsubsection{Target Estimate Calculation}
The target estimate is the result of applying discounting to accumulated reward.
The discount factor ($\gamma$), configured using the \soarb{discount-rate} parameter, allows the agent to value immediate rewards over more temporally distant rewards.
The value that is chosen for discount is configured by the \soarb{learning-policy} parameter.

\subsection{Gaps in Rule Coverage}
\label{RL-gaps}

As described, RL updates are transmitted backwards through time, between subsequent decisions. However, requiring that RL rules support operators at every decision can be difficult for agent programmers, particularly when operators are required that do not represent steps in a task, but instead perform generic maintenance functions, such as cleaning processed output-link structures.

To address this practical issue, RL supports automatic propagation of updates over "gaps," defined as one or more contiguous decision cycles during which no RL rules fire. By default, Soar will automatically propagate updates over gaps, discounted exponentially by the \soarb{discount-rate} parameter, with respect to the length of the gap (defined as the number of decision cycles). This behavior can be enabled/disabled by manipulating the \soarb{temporal-extension} parameter. If the \soarb{temporal-extension} parameter is set to \soarb{off}, no updates will propagate across gaps. The \soarb{rl} setting of the \soarb{watch} command (see Section \ref{watch} on page \pageref{watch}) is useful in identifying gaps.

\subsection{Eligibility Traces}
\label{RL-et}
The RL mechanism supports eligibility traces, which can improve the speed of learning by updating RL rules across multiple sequential steps. The \soarb{eligibility-trace-decay-rate} and \soarb{eligibility-trace-tolerance} parameters control this mechanism. By setting \soarb{eligibility-trace-decay-rate} to \soarb{0} (default), eligibility traces are in effect disabled. When eligibility traces are enabled, the particular algorithm used is dependent upon the current value of the \soarb{learning-policy} parameter. If the learning policy is \soarb{sarsa}, the eligibility trace implementation is \emph{Sarsa($\lambda$)}; whereas if the learning policy is \soarb{q-learning}, the eligibility trace implementation is \emph{Watkin's Q($\lambda$)}.

\subsection{Hierarchical Learning}
\label{RL-hrl}

Hierarchical reinforcement learning (HRL) is reinforcement learning done over a hierarchically decomposed task structure. Learning can be applied both to improving the implementation of a subtask and to the selection among subtasks. Compared to flat RL, HRL can demonstrate faster learning on a single task and can learn policies that are easier to transfer to related tasks. As applied to Soar, HRL relates to how the RL mechanism behaves during subgoal processing.

\subsubsection{Operator No-Change Impasses}
Consider the operator trace in Figure \ref{fig:rl-optrace} where operator \soarb{O1} is selected and impassed at state \soarb{S1}, with operators \soarb{O11}, \soarb{O12}, and \soarb{O13} being selected and applied at substate \soarb{S2}:

\begin{figure}
\insertfigure{Figures/rl-optrace}{1.5in}
\insertcaption{Example Soar subgoal operator trace.}
\label{fig:rl-optrace}
\end{figure}

The reinforcement learning algorithm will treat the operator selection at \soarb{S1} and \soarb{S2} as two separate RL problems (\soarb{S1} and \soarb{S2} have independent reward-link structures):

\begin{list}{S1}
\item Rewards at \soarb{S1} while \soarb{O1} is impassed are attributed to \soarb{O1}. By default, these rewards and the next-state prediction are discounted by the number of decision cycles that \soarb{O1} has been impassed.  So if rewards \soarb{r1}, \soarb{r2}, and \soarb{r3} are the rewards received at \soarb{S1} while \soarb{O1} is impassed, the target estimate for \soarb{Q(S1, O1)} is

$$r1 + \gamma(r2) + \gamma^2(r3) + \gamma^3[  Q(S1, O2)  ]$$

This model maintains the definition of the Q-function as representing the expected discounted sum of future reward received after selecting an operator.

Setting the \soarb{hrl-discount} parameter to \soarb{off} will change this behavior, such that the number of cycles \soarb{O1} has been impassed will be ignored.  Thus the target estimate for \soarb{Q(S1, O1)} would be

$$r1 + r2 + r3 + \gamma[  Q(S1, O2)  ]$$

\end{list}

\begin{list}{S2}
\item After applying \soarb{O13}, immediately before the state \soarb{S2} is removed, the architecture checks for reward \soarb{r} at \soarb{S2}. The target estimate for \soarb{Q(S2, O13)} is just \soarb{r}.

\end{list}

\subsubsection{Other Soar Impasses}
For impasses other than operator no-change, the behavior of RL at top-state \soarb{S1} and substate \soarb{S2} is as follows:

\begin{list}{S1}
\item During these impasses, there is no operator installed at \soarb{S1}.  If \soarb{O1} is the last operator selected before the impasse, \soarb{r} the reward received in the decision cycle immediately following \soarb{O1}, and \soarb{O2} the first operator selected after the impasse, then \soarb{O1} is updated with the target \soarb{r} + $\gamma$[\soarb{Q(S1, O2)}]. In other words, RL acts as if the impasse hadn't occurred.
\end{list}

\begin{list}{S2}
\item RL acts exactly as it does for an operator no-change: the substate is treated as an episodic task.

\end{list}

\section{Automatic Generation of RL Rules}

In the basic RL use case, the programmer would like the agent to learn to choose the optimal operator in each possible state of the environment.
The most straightforward way to achieve this is to give the agent a set of RL rules, each of which matches exactly one possible state-operator pair.
This approach is equivalent to a table-based RL algorithm, where the numeric indifferent preference asserted by each RL rule corresponds to the Q-value of one state-action pair.
Even for small environments, this approach can require a large number of rules.
It would be undesirable to write each rule by hand, so several methods exist to automate this.

\subsection{The gp Command}
The \soar{gp} command can be used to generate productions based on simple patterns.
This is useful if the states and operators of the environment can be distinguished by a fixed number of dimensions with finite domains.
An example is a grid world where the states are described by integer row/column coordinates, and the available operators are to move north, south, east, or west.
In this case, a single \soar{gp} command will generate all necessary RL rules:
	
\begin{verbatim}
gp {gen*rl*rules
   (state <s> ^name gridworld
              ^operator <o> +
              ^row [ 1 2 3 4 ]
              ^col [ 1 2 3 4 ])
   (<o> ^name move
        ^direction [ north south east west ])
-->
   (<s> ^operator <o> = 0.0)
}
\end{verbatim}
	
For more information see the documentation for this command on page \pageref{gp}.

\subsection{Rule Templates}
\label{RL-templates}

Rule templates allow Soar to dynamically generate new RL rules based on a predefined pattern as the agent encounters novel states.
This is useful when either the domains of environment dimensions are not known ahead of time, or when the enumerable state space of the environment is too large to capture in its entirety using \soar{gp}, but the agent will only encounter a small fraction of that space during its execution.
For example, consider the grid world example with 1000 rows and columns.
Attempting to generate RL rules for each grid cell and action a priori will result in $1000 \times 1000 \times 4 = 4 \times 10^6$ productions.
However, if most of those cells are unreachable due to walls, then the agent will never fire or update most of those productions.
Templates give the programmer the convenience of the \soar{gp} command without filling production memory with unnecessary rules.

Rule templates have variables that are filled in to generate RL rules as the agent encounters novel combinations of variable values.
A rule template is valid if and only if it is marked with the \soarb{:template} flag and, in all other respects, adheres to the format of an RL rule.
However, whereas an RL rule may only use constants as the numeric indifference preference value, a rule template may use a variable.
Consider the following rule template:

\begin{verbatim}
sp {sample*rule*template
    :template
    (state <s> ^operator <o> +
               ^value <v>)
-->
    (<s> ^operator <o> = <v>)
}
\end{verbatim}

During agent execution, this rule template will match working memory and fire like any other rule.
However, the rule firing will not create the numeric indifferent preference on the RHS.
Instead, a new production is created by substituting all variables in the rule template that matched against constant values with the values themselves.
Suppose that the LHS of the rule template matched against the state

\begin{verbatim}
(S1 ^value 3.2)
(S1 ^operator O1 +)
\end{verbatim}

Then the following production will be added to production memory:

\begin{verbatim}
sp {rl*sample*rule*template*1
    (state <s> ^operator <o> +
               ^value 3.2)
-->
    (<s> ^operator <o> = 3.2)
}
\end{verbatim}

The variable \soar{<v>} is replaced by \soar{3.2} on both the LHS and the RHS, but \soar{<s>} and \soar{<o>} are not replaced because they matches against identifiers (\soar{S1} and \soar{O1}).
As with other RL rules, the value of \soar{3.2} on the RHS of this rule may be updated later by reinforcement learning, whereas the value of \soar{3.2} on the LHS will remain unchanged.
If \soar{<v>} had matched against a non-numeric constant, it will be replaced by that constant on the LHS, but the RHS numeric indifference preference value will be set to zero to make the new rule valid.

The new production's name adheres to the following pattern:
\soarb{rl*template-name*id}, where \soarb{template-name} is the name of the originating rule template and \soarb{id} is the smallest positive integer such that the new production's name is unique.

If an identical production already exists in production memory, then the newly generate production is discarded.
It should be noted that the current process of identifying unique template match instances can become quite expensive in long agent runs.
Therefore, it is recommended to generate all necessary RL rules using the \soar{gp} command or via custom scripting when possible.

\subsection{Chunking}
Since RL rules are regular productions, they can be learned by chunking just like any other production.
This method is more general than using the \soar{gp} command or rule templates, and is useful if the environment state consists of arbitrarily complex relational structures that cannot be enumerated.
