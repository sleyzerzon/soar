#summary rl command documentation

= rl =

Control how numeric indifferent preference values in RL rules are
updated via reinforcement learning.

== Synopsis ==

{{{
rl -g|--get <parameter>
rl -s|--set <parameter> <value>
rl -S|--stats <statistic>
}}}

== Options ==

|| `-g, --get` || Print current parameter setting ||
|| `-s, --set` || Set parameter value ||
|| `-S, --stats` || Print statistic summary or specific statistic ||

== Description ==

The `rl` command sets parameters and displays information related to
reinforcement learning.  The `print` and `watch` commands display
additional RL related information not covered by this command.

=== Parameters ===

Due to the large number of parameters, the `rl` command uses the
`--get|--set <parameter> <value>` convention rather than individual
switches for each parameter.  Running `rl` without any switches
displays a summary of the parameter settings.

|| `apoptosis` || Automatic excising of productions via base-level decay  || `none`, `chunks`, `rl-chunks` || `none` ||
|| `apoptosis-decay` || Base-level decay parameter || `[`0, 1`]` || 0.5 ||
|| `apoptosis-thresh` || Base-level threshold parameter (negates supplied value) || (0, inf) || 2 ||
|| `chunk-stop` || If enabled, chunking does not create duplicate RL rules that differ only in numeric-indifferent preference value || `on`, `off` || `on` ||
|| `discount-rate` || Temporal discount (gamma) || `[`0, 1`]` || 0.9 ||
|| `eligibility-trace-decay-rate` || Eligibility trace decay factor (lambda) || `[`0, 1`]` || 0 ||
|| `eligibility-trace-tolerance` || Smallest eligibility trace value not considered 0 || (0, inf) || 0.001 ||
|| `hrl-discount` || Discounting of RL updates over time in impassed states || `on`, `off` || `off` ||
|| `learning` || Reinforcement learning enabled || `on`, `off` || `off` ||
|| `learning-rate` || Learning rate (alpha) || `[`0, 1`]` || 0.3 ||
|| `learning-policy` || Value update policy || `sarsa`, `q-learning` || `sarsa` ||
|| `temporal-discount` || Discount RL updates over gaps  || `on`, `off` || `on` ||
|| `temporal-extension` || Propagation of RL updates over gaps  || `on`, `off` || `on` ||

Apoptosis is a process to automatically excise chunks via the base-level decay model (where rule firings are the activation events).
A value of `chunks` has this apply to any chunk, whereas `rl-chunks` means only chunks that are also RL rules can be forgotten.

=== Statistics ===

Soar tracks some RL statistics over the lifetime of the agent.  These
can be accessed using `rl --stats <statistic>`.  Running `rl --stats`
without a statistic will list the values of all statistics.

|| `update-error` || Difference between target and current values in last RL update ||
|| `total-reward` || Total accumulated reward in the last update ||
|| `global-reward` || Total accumulated reward since agent initialization ||

== See Also ==

[cmd_excise excise] [cmd_print print] [cmd_watch watch]